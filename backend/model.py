# backend/model.py (–í–µ—Ä—Å–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π 'kk' –º–æ–¥–µ–ª–∏)

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, XLMRobertaTokenizer
from typing import Dict, List
import logging
import os

logger = logging.getLogger(__name__)

class FakeNewsDetector:
    """
    –ö–ª–∞—Å—Å-–º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏.
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏-"—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã"
    (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'en', 'kk') –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –Ω—É–∂–Ω—É—é
    –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞.
    –¢–∞–∫–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç NLI –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
    """
    def __init__(self, device: str = None):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        self.classifier_models: Dict[str, AutoModelForSequenceClassification] = {}
        self.classifier_tokenizers: Dict[str, AutoTokenizer] = {}
        # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ, –≥–¥–µ –ª–µ–∂–∞—Ç –ø–∞–ø–∫–∏ —Å –º–æ–¥–µ–ª—è–º–∏ (truthlens_en_model, truthlens_kk_model)
        models_base_path = "backend/models"

        # --- –®–ê–ì 1: –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –í–°–ï–• –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–û–ù–ù–´–• –ú–û–î–ï–õ–ï–ô ---
        # –°–∫–∞–Ω–∏—Ä—É–µ—Ç –ø–∞–ø–∫—É models_base_path –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Ö–æ–¥–∏—Ç.
        # –û–∂–∏–¥–∞—é—Ç—Å—è –ø–∞–ø–∫–∏ –≤–∏–¥–∞ 'truthlens_xx_model', –≥–¥–µ xx - –∫–æ–¥ —è–∑—ã–∫–∞ (en, kk, ru –∏ —Ç.–¥.)
        logger.info(f"–ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤: {models_base_path}")
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –±–∞–∑–æ–≤–∞—è –ø–∞–ø–∫–∞
            if not os.path.isdir(models_base_path):
                 logger.warning(f"–ü–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—è–º–∏ '{models_base_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            else:
                for item_name in os.listdir(models_base_path):
                    model_path = os.path.join(models_base_path, item_name)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–∞–ø–∫–∞, —á—Ç–æ –∏–º—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É –∏ –µ—Å—Ç—å config.json
                    if (os.path.isdir(model_path) and
                        item_name.startswith("truthlens_") and
                        item_name.endswith("_model") and
                        "config.json" in os.listdir(model_path)):

                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ —è–∑—ã–∫–∞ –∏–∑ –∏–º–µ–Ω–∏ –ø–∞–ø–∫–∏
                        try:
                             # –ù–∞–ø—Ä–∏–º–µ—Ä, 'truthlens_kk_model' -> 'kk'
                            lang_code = item_name.split('_')[1]
                            if len(lang_code) != 2: # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —ç—Ç–æ –¥–≤—É—Ö–±—É–∫–≤–µ–Ω–Ω—ã–π –∫–æ–¥
                                raise ValueError("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∫–æ–¥ —è–∑—ã–∫–∞ –≤ –∏–º–µ–Ω–∏ –ø–∞–ø–∫–∏")
                        except (IndexError, ValueError):
                             logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–¥ —è–∑—ã–∫–∞ –∏–∑ –∏–º–µ–Ω–∏ –ø–∞–ø–∫–∏ '{item_name}'. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                             continue

                        logger.info(f"–ù–∞–π–¥–µ–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è —è–∑—ã–∫–∞ '{lang_code}'. –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑: {model_path}")
                        try:
                            tokenizer = AutoTokenizer.from_pretrained(model_path)
                            model = AutoModelForSequenceClassification.from_pretrained(model_path)
                            model.to(self.device)
                            model.eval() # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏

                            self.classifier_models[lang_code] = model
                            self.classifier_tokenizers[lang_code] = tokenizer
                            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –¥–ª—è —è–∑—ã–∫–∞ '{lang_code}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
                        except Exception as load_err:
                             logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è —è–∑—ã–∫–∞ '{lang_code}' –∏–∑ {model_path}: {load_err}", exc_info=True)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å –ª–∏ —Ö–æ—Ç—å –∫–∞–∫–∏–µ-—Ç–æ –º–æ–¥–µ–ª–∏
            if not self.classifier_models:
                logger.error("üõë –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ 'backend/models/'.")
                # –ú–æ–∂–Ω–æ –∑–¥–µ—Å—å –≤—ã–±—Ä–æ—Å–∏—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª–æ—Å—å
                # raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.")
            else:
                 logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è —è–∑—ã–∫–æ–≤: {list(self.classifier_models.keys())}")


            # --- –®–ê–ì 2: –ó–ê–ì–†–£–ó–ö–ê NLI –ú–û–î–ï–õ–ò –î–õ–Ø –†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–Ø –ò–°–¢–û–ß–ù–ò–ö–û–í ---
            # –≠—Ç–∞ –º–æ–¥–µ–ª—å –æ—Å—Ç–∞–µ—Ç—Å—è –æ–¥–Ω–∞, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∞ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è (XLM-R)
            nli_model_name = "joeddav/xlm-roberta-large-xnli"
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ NLI –º–æ–¥–µ–ª–∏: {nli_model_name}")
            try:
                self.nli_tokenizer = XLMRobertaTokenizer.from_pretrained(nli_model_name)
                self.nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name)
                self.nli_model.to(self.device)
                self.nli_model.eval()
                logger.info("‚úÖ NLI –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
            except Exception as nli_err:
                 logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å NLI –º–æ–¥–µ–ª—å {nli_model_name}: {nli_err}", exc_info=True)
                 # –ï—Å–ª–∏ NLI –º–æ–¥–µ–ª—å –≤–∞–∂–Ω–∞, –º–æ–∂–Ω–æ —Ç–æ–∂–µ –≤—ã–±—Ä–æ—Å–∏—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
                 # raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å NLI –º–æ–¥–µ–ª—å.")
                 self.nli_tokenizer = None
                 self.nli_model = None


        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ FakeNewsDetector: {e}", exc_info=True)
            raise # –ü–µ—Ä–µ–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã FastAPI –∑–Ω–∞–ª –æ –ø—Ä–æ–±–ª–µ–º–µ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ

    def predict(self, text: str, language: str) -> Dict:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–ª–∞—Å—Å —Ç–µ–∫—Å—Ç–∞ ('real' –∏–ª–∏ 'fake'), –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å
        –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.

        Args:
            text (str): –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
            language (str): –ö–æ–¥ —è–∑—ã–∫–∞ ('en', 'kk', 'ru', ...).

        Returns:
            Dict: –°–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏ 'classification' ('real', 'fake' –∏–ª–∏ 'uncertain')
                  –∏ 'confidence' (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞).
                  –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å 'classification': 'error'.
        """
        # --- –í–´–ë–û–† –ù–£–ñ–ù–û–ì–û –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê ---
        model = self.classifier_models.get(language)
        tokenizer = self.classifier_tokenizers.get(language)

        if not model or not tokenizer:
            logger.warning(f"–ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è —è–∑—ã–∫–∞ '{language}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
            # --- –í–ê–†–ò–ê–ù–¢ –û–ë–†–ê–ë–û–¢–ö–ò –û–¢–°–£–¢–°–¢–í–ò–Ø –ú–û–î–ï–õ–ò ---
            # –ú–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ:
            # 1. –í–µ—Ä–Ω—É—Ç—å "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π" —Ä–µ–∑—É–ª—å—Ç–∞—Ç:
            return {"classification": "uncertain", "confidence": 0.5}
            # 2. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'en'):
            # default_lang = 'en'
            # if default_lang in self.classifier_models:
            #     logger.warning(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: '{default_lang}'")
            #     model = self.classifier_models[default_lang]
            #     tokenizer = self.classifier_tokenizers[default_lang]
            # else:
            #     logger.error(f"–ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é '{default_lang}' —Ç–∞–∫–∂–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
            #     return {"classification": "error", "confidence": 0.0, "error": "No suitable model found"}
            # 3. –í–µ—Ä–Ω—É—Ç—å —è–≤–Ω—É—é –æ—à–∏–±–∫—É (—Ç–µ–∫—É—â–∏–π –∫–æ–¥ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ –Ω–∏–∂–µ —á–µ—Ä–µ–∑ raise)

        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è —è–∑—ã–∫–∞: {language}")
        try:
            inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad(): # –û—Ç–∫–ª—é—á–∞–µ–º —Ä–∞—Å—á–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                outputs = model(**inputs)
                probabilities = torch.softmax(outputs.logits, dim=-1)[0] # –ë–µ—Ä–µ–º [0], —Ç–∞–∫ –∫–∞–∫ –±–∞—Ç—á=1

            # --- –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞ ---
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º id2label –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
            id2label = model.config.id2label if hasattr(model.config, 'id2label') else {0: 'REAL', 1: 'FAKE'} # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            predicted_class_id = probabilities.argmax().item()

            label_map = {v.upper(): k for k, v in id2label.items()} # {'REAL': 0, 'FAKE': 1}
            fake_label_id = label_map.get('FAKE', 1) # –ò—â–µ–º ID –¥–ª—è FAKE, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1

            fake_prob = probabilities[fake_label_id].item()
            real_prob = probabilities[label_map.get('REAL', 0)].item()

            if fake_prob > real_prob:
                 classification = "fake"
                 confidence = fake_prob
            else:
                 classification = "real"
                 confidence = real_prob

            logger.debug(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ({language}): fake_prob={fake_prob:.4f}, real_prob={real_prob:.4f} -> {classification} (conf: {confidence:.4f})")

            return {
                "classification": classification,
                "confidence": confidence,
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è —è–∑—ã–∫–∞ '{language}': {e}", exc_info=True)
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫—É, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å app.py
            # –í–∞–∂–Ω–æ: –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–π –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫—É, –∞ —Å–ª–æ–≤–∞—Ä—å, —á—Ç–æ–±—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –æ–∂–∏–¥–∞–µ–º–æ–º—É —Ç–∏–ø—É Dict
            return {"classification": "error", "confidence": 0.0, "error": f"Prediction failed for lang {language}"}


    def rank_sources_nli(self, query_text: str, search_results: List[Dict]) -> List[Dict]:
        """
        –†–∞–Ω–∂–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ (—Å–ª–æ–≤–∞—Ä–∏ —Å 'snippet', 'url', 'title')
        –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ query_text —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLI –º–æ–¥–µ–ª–∏.
        –î–æ–±–∞–≤–ª—è–µ—Ç –∫–ª—é—á 'relevance' –∫ –∫–∞–∂–¥–æ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫.
        """
        if not self.nli_model or not self.nli_tokenizer:
             logger.warning("NLI –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ.")
             return search_results # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å

        if not search_results:
            return []

        ranked_results = []
        # –ü–æ–ª—É—á–∞–µ–º ID –¥–ª—è –º–µ—Ç–æ–∫ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ NLI –º–æ–¥–µ–ª–∏
        # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (0, 1, 2) –≤–∑—è—Ç—ã –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ xnli
        entailment_id = self.nli_model.config.label2id.get('entailment', 2)
        contradiction_id = self.nli_model.config.label2id.get('contradiction', 0)
        neutral_id = self.nli_model.config.label2id.get('neutral', 1)

        logger.debug(f"NLI label IDs: Entailment={entailment_id}, Contradiction={contradiction_id}, Neutral={neutral_id}")

        for result in search_results:
            snippet = result.get("snippet") or result.get("description") or "" # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏ snippet, –∏ description
            if len(snippet) < 15: # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è
                 logger.debug(f"–ü—Ä–æ–ø—É—Å–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑-–∑–∞ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Å–Ω–∏–ø–ø–µ—Ç–∞: {result.get('url')}")
                 continue

            # NLI –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç –ø–∞—Ä—É: (premise, hypothesis)
            # premise - —ç—Ç–æ —Ç–µ–∫—Å—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (snippet), hypothesis - —ç—Ç–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ (query_text)
            premise, hypothesis = snippet, query_text

            try:
                inputs = self.nli_tokenizer(premise, hypothesis, return_tensors="pt", truncation=True, max_length=256).to(self.device)

                with torch.no_grad():
                    outputs = self.nli_model(**inputs)
                    probabilities = torch.softmax(outputs.logits, dim=-1)[0]

                # –°—á–∏—Ç–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∫ (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è)
                # –≠—Ç–æ –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–±, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏ –¥—Ä—É–≥–∏–µ –º–µ—Ç—Ä–∏–∫–∏
                relevance = probabilities[entailment_id].item() - probabilities[contradiction_id].item()
                # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏ neutral_prob –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                # neutral_prob = probabilities[neutral_id].item()
                # logger.debug(f"NLI scores for '{result.get('title', '')[:30]}...': E={probabilities[entailment_id]:.3f}, C={probabilities[contradiction_id]:.3f}, N={neutral_prob:.3f} -> Relevance={relevance:.3f}")


                result_copy = result.copy()
                result_copy["relevance"] = relevance
                ranked_results.append(result_copy)
            except Exception as nli_pred_err:
                 logger.warning(f"–û—à–∏–±–∫–∞ NLI –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {result.get('url')}: {nli_pred_err}", exc_info=False) # –ù–µ –ª–æ–≥–∏—Ä—É–µ–º –≤–µ—Å—å —Ç—Ä–µ–π—Å–±–µ–∫
                 continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        ranked_results.sort(key=lambda x: x.get("relevance", -1.0), reverse=True)

        logger.info(f"–†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–æ {len(ranked_results)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é NLI.")
        return ranked_results