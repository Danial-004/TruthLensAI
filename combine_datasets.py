# combine_datasets.py (–ù–û–í–ê–Ø –í–ï–†–°–ò–Ø –î–õ–Ø –†–ê–ó–î–ï–õ–¨–ù–´–• –î–ê–¢–ê–°–ï–¢–û–í)
import pandas as pd
import os
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

DATA_FOLDER = "training_data"
# –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
OUTPUT_FOLDER = "processed_data"
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
FILE_CONFIGS = {
    'en_real': {'path': 'True.csv', 'text_col': 'text', 'label': 0, 'lang': 'en', 'sep': ','},
    'en_fake': {'path': 'Fake.csv', 'text_col': 'text', 'label': 1, 'lang': 'en', 'sep': ','},
    'ru': {
        'path': 'russian_news_dataset.csv',
        'text_col': 'title',
        'label_col': 'is_fake',
        'lang': 'ru',
        'sep': '\t'
    },
    'kz_real': {'path': 'tengri_news.csv', 'text_col': 'text', 'label': 0, 'lang': 'kz', 'sep': ','},
    'kz_fake': {'path': 'kazakhfakedata_clean.csv', 'text_col': 'text', 'label': 1, 'lang': 'kz', 'sep': ','},
}

def process_and_save_datasets():
    """
    –ß–∏—Ç–∞–µ—Ç –≤—Å–µ –∏—Å—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã, –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –∏—Ö –ø–æ —è–∑—ã–∫–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç
    –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ, –æ—á–∏—â–µ–Ω–Ω—ã–µ CSV —Ñ–∞–π–ª—ã.
    """
    all_data = []
    logging.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")

    for key, config in FILE_CONFIGS.items():
        filepath = os.path.join(DATA_FOLDER, config['path'])
        try:
            logging.info(f"–ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª: {filepath}")
            df = pd.read_csv(filepath, sep=config['sep'], on_bad_lines='warn')

            if 'label_col' in config: # –î–ª—è —Ñ–∞–π–ª–æ–≤, –≥–¥–µ –º–µ—Ç–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ (russian_news)
                df_processed = df[[config['text_col'], config['label_col']]].rename(columns={
                    config['text_col']: 'text',
                    config['label_col']: 'label'
                })
            else: # –î–ª—è —Ñ–∞–π–ª–æ–≤, –≥–¥–µ –º–µ—Ç–∫–∏ –º—ã –∑–∞–¥–∞–µ–º —Å–∞–º–∏ (True.csv, Fake.csv)
                df_processed = df[[config['text_col']]].rename(columns={config['text_col']: 'text'})
                df_processed['label'] = config['label']

            df_processed['language'] = config['lang']
            all_data.append(df_processed)
            logging.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(df_processed)} —Å—Ç—Ä–æ–∫ –∏–∑ {config['path']}")

        except FileNotFoundError:
            logging.warning(f"‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º: {filepath}")
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {filepath}: {e}")

    if not all_data:
        logging.error("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
        return

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤ –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    final_df = pd.concat(all_data, ignore_index=True)
    final_df.dropna(subset=['text'], inplace=True) # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞
    final_df = final_df[final_df['text'].str.len() > 50] # –£–¥–∞–ª—è–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
    final_df['label'] = final_df['label'].astype(int)

    logging.info(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {len(final_df)}")

    # –†–∞–∑–¥–µ–ª—è–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ —è–∑—ã–∫–∞–º
    for lang_code in final_df['language'].unique():
        lang_df = final_df[final_df['language'] == lang_code]
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞
        lang_df = lang_df.sample(frac=1).reset_index(drop=True)
        
        output_path = os.path.join(OUTPUT_FOLDER, f"dataset_{lang_code}.csv")
        lang_df.to_csv(output_path, index=False)
        logging.info(f"üíæ –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è —è–∑—ã–∫–∞ '{lang_code}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_path} ({len(lang_df)} —Å—Ç—Ä–æ–∫)")

if __name__ == "__main__":
    process_and_save_datasets()