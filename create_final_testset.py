import pandas as pd
import os

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
# ‚ùóÔ∏è –ò–°–¢–û–ß–ù–ò–ö –§–ï–ô–ö–û–í - –¢–ï–ü–ï–†–¨ –¢–í–û–ô –§–ê–ô–õ –° GPT-–§–ï–ô–ö–ê–ú–ò
FILE_FAKES_SOURCE = "generated_fakes_kz_100.csv"
# ‚ùóÔ∏è –ò–°–¢–û–ß–ù–ò–ö–ò –†–ï–ê–õ–¨–ù–´–• - –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ï –§–ê–ô–õ–´
FILE_REAL_EGEMEN = "dataset_kz.csv"
FILE_REAL_TENGRI = "tengri_news.csv"

# –§–∞–π–ª —Å –æ–±—É—á–∞—é—â–∏–º–∏/–≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (—á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å –∏—Ö)
TRAINING_DATA_FILE = "final_golden_dataset.csv"
# –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä
OUTPUT_FILE = "final_test_set_100x100_v5_gpt.csv" # –ù–æ–≤–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ v5

# –°–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞/–∏—Å—Ç–æ—á–Ω–∏–∫–∞
NUM_FAKES_NEEDED = 100 # –í—Å–µ–≥–æ 100 —Ñ–µ–π–∫–æ–≤
NUM_REAL_PER_SOURCE = 50 # –ü–æ 50 —Ä–µ–∞–ª—å–Ω—ã—Ö —Å –∫–∞–∂–¥–æ–≥–æ —Å–∞–π—Ç–∞
# --------------------

TEXT_COLUMN = "text"   # –û–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∏–º—è –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
LABEL_COLUMN = 'label'

print("--- –°–æ–∑–¥–∞–Ω–∏–µ –§–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¢–µ—Å—Ç–æ–≤–æ–≥–æ –ù–∞–±–æ—Ä–∞ v5 (100 GPT-Fake + 100 Real) ---")

# --- 1. –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö (—á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å –∏—Ö) ---
try:
    df_train = pd.read_csv(TRAINING_DATA_FILE)
    seen_texts = set(df_train[TEXT_COLUMN].dropna())
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(seen_texts)} —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ/–≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è.")
except FileNotFoundError:
    print(f"üõë –û–®–ò–ë–ö–ê: –§–∞–π–ª –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö '{TRAINING_DATA_FILE}' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    exit()

# --- 2. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏, –æ—á–∏—Å—Ç–∫–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ ---
def load_filter_sample(filename, text_col, num_samples, output_label_val, seen_texts_set, filter_label_str=None):
    print(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {filename}...")
    if not os.path.exists(filename):
        print(f"‚ö†Ô∏è –§–∞–π–ª '{filename}' –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        return pd.DataFrame()

    try:
        df = pd.read_csv(filename)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        if text_col not in df.columns or (filter_label_str is not None and LABEL_COLUMN not in df.columns):
             print(f"‚ö†Ô∏è –í —Ñ–∞–π–ª–µ {filename} –Ω–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ '{text_col}' –∏–ª–∏ '{LABEL_COLUMN}', –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
             return pd.DataFrame()

        df.dropna(subset=[text_col, LABEL_COLUMN], inplace=True)
        df = df[df[text_col].str.len() > 50]

        # --- –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–û –ú–ï–¢–ö–ï (–¥–ª—è —Ñ–∞–π–ª–∞ —Å —Ñ–µ–π–∫–∞–º–∏ –æ—Ç GPT) ---
        if filter_label_str is not None:
            df[LABEL_COLUMN] = df[LABEL_COLUMN].astype(str) # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç—Ä–æ–∫–µ
            df = df[df[LABEL_COLUMN] == filter_label_str]
            print(f"  –ù–∞–π–¥–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ —Å –º–µ—Ç–∫–æ–π '{filter_label_str}'.")

        # --- –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ–∫—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏ ---
        original_count = len(df)
        df = df[~df[text_col].isin(seen_texts_set)]
        filtered_count = len(df)
        print(f"  –ò—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫ (—Å –Ω—É–∂–Ω–æ–π –º–µ—Ç–∫–æ–π): {original_count}. –°—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è '–≤–∏–¥–µ–Ω–Ω—ã—Ö': {filtered_count}.")

        if filtered_count < num_samples:
            print(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫ ({filtered_count} < {num_samples}). –ë–µ—Ä–µ–º –≤—Å–µ, —á—Ç–æ –µ—Å—Ç—å.")
            num_samples = filtered_count

        if num_samples == 0:
            print(f"  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å—Ç—Ä–æ–∫.")
            return pd.DataFrame()

        df_sample = df.sample(n=num_samples, random_state=42)
        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫—É –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –Ω—É–∂–Ω—É—é –º–µ—Ç–∫—É
        df_sample = df_sample[[text_col]].rename(columns={text_col: TEXT_COLUMN})
        df_sample[LABEL_COLUMN] = output_label_val
        print(f"  –í–∑—è—Ç–æ {len(df_sample)} —Å–ª—É—á–∞–π–Ω—ã—Ö —Å—Ç—Ä–æ–∫.")
        return df_sample[[TEXT_COLUMN, LABEL_COLUMN]]

    except Exception as e:
        print(f"‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {filename}: {e}. –§–∞–π–ª –ø—Ä–æ–ø—É—â–µ–Ω.")
        return pd.DataFrame()

# --- 3. –°–æ–±–∏—Ä–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä ---
all_test_dfs = []

# –§–µ–π–∫–∏ (label=0) –∏–∑ generated_fakes_kz_100.csv
# –í–∞–∂–Ω–æ: –í —Ñ–∞–π–ª–µ –æ—Ç GPT –º–µ—Ç–∫–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å '0' (—Å—Ç—Ä–æ–∫–∞)
df_fakes = load_filter_sample(FILE_FAKES_SOURCE, TEXT_COLUMN, NUM_FAKES_NEEDED, 0, seen_texts, filter_label_str='0')
all_test_dfs.append(df_fakes)

# –†–µ–∞–ª—å–Ω—ã–µ (label=1)
df_real_e = load_filter_sample(FILE_REAL_EGEMEN, TEXT_COLUMN, NUM_REAL_PER_SOURCE, 1, seen_texts)
all_test_dfs.append(df_real_e)
df_real_t = load_filter_sample(FILE_REAL_TENGRI, TEXT_COLUMN, NUM_REAL_PER_SOURCE, 1, seen_texts)
all_test_dfs.append(df_real_t)

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ß–ê–°–¢–ò, –ö–û–¢–û–†–´–ï –£–î–ê–õ–û–°–¨ –ó–ê–ì–†–£–ó–ò–¢–¨
valid_dfs = [df for df in all_test_dfs if df is not None and not df.empty]
if not valid_dfs:
     print("üõë –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–∞. –ü—Ä–æ–≤–µ—Ä—å –∏—Å—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã.")
     exit()

df_test_final = pd.concat(valid_dfs, ignore_index=True)

# –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
df_test_final = df_test_final.sample(frac=1, random_state=42).reset_index(drop=True)

# --- 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---
df_test_final.to_csv(OUTPUT_FILE, index=False, encoding='utf-8', quoting=1)

print("\n--- –ì–û–¢–û–í–û! ---")
print(f"–°–æ–∑–¥–∞–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä: {OUTPUT_FILE}")
print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(df_test_final)}")
print(f"–§–µ–π–∫–æ–≤ (label 0): {len(df_test_final[df_test_final[LABEL_COLUMN] == 0])}")
print(f"–†–µ–∞–ª—å–Ω—ã—Ö (label 1): {len(df_test_final[df_test_final[LABEL_COLUMN] == 1])}")