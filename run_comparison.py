import pandas as pd
import os
# sklearn –∏–º–ø–æ—Ä—Ç—Ç–∞—Ä—ã –±“±–ª –∂–µ—Ä–¥–µ “õ–∞–∂–µ—Ç –µ–º–µ—Å, —Å–µ–±–µ–±—ñ ML –º–æ–¥–µ–ª—ñ–Ω —Ç–µ—Å—Ç—ñ–ª–µ–º–µ–π–º—ñ–∑
from sklearn.metrics import classification_report
from transformers import pipeline # DL –º–æ–¥–µ–ª—å–¥—ñ –∂“Ø–∫—Ç–µ—É “Ø—à—ñ–Ω
import torch
import warnings
from tqdm import tqdm

# --- –ü–∞—Ä–∞–º–µ—Ç—Ä–ª–µ—Ä ---
# ‚úÖ‚úÖ‚úÖ ”®–ó–ì–ï–†–Ü–°: –ñ–∞“£–∞ —Ç–µ—Å—Ç —Ñ–∞–π–ª—ã–Ω—ã“£ –∞—Ç—ã–Ω –∫”©—Ä—Å–µ—Ç—Ç—ñ–∫ ‚úÖ‚úÖ‚úÖ
FACT_FAKES_TEST_DATASET_PATH = "test_fact_fakes.csv"
relative_model_path = os.path.join("backend", "models", "truthlens_kk_model") # DL –º–æ–¥–µ–ª—å
MODEL_PATH = os.path.abspath(relative_model_path)
DEVICE = 0 if torch.cuda.is_available() else -1 # DL “Ø—à—ñ–Ω

# –ï—Å–∫–µ—Ä—Ç—É–ª–µ—Ä–¥—ñ ”©—à—ñ—Ä—É
warnings.filterwarnings("ignore")
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ‚úÖ‚úÖ‚úÖ ”®–ó–ì–ï–†–Ü–°: –¢–µ—Å—Ç —Ñ–∞–π–ª—ã–Ω–¥–∞ 'real'/'fake' –∫“Ø—Ç–µ–º—ñ–∑ ‚úÖ‚úÖ‚úÖ
label_map_test = {'real': 1, 'fake': 0}

# DL –º–æ–¥–µ–ª—å –±–æ–ª–∂–∞–º–¥–∞—Ä—ã–Ω —Å–∞–Ω–¥–∞—Ä“ì–∞ –∞—É—ã—Å—Ç—ã—Ä—É —Ñ—É–Ω–∫—Ü–∏—è—Å—ã (”©–∑–≥–µ—Ä—ñ—Å—Å—ñ–∑)
def parse_dl_label(d):
    return 1 if d['label'] == 'LABEL_1' else 0

print(f"--- –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢: –§–ê–ö–¢-–§–ï–ô–ö–¢–ï–†–î–Ü –¢–ï–°–¢–Ü–õ–ï–£ ({FACT_FAKES_TEST_DATASET_PATH}) ---")

print("--- –î–ï–†–ï–ö–¢–ï–†–î–Ü –ñ“Æ–ö–¢–ï–£ ---")
X_test = []
y_test = []
try:
    df_test = pd.read_csv(FACT_FAKES_TEST_DATASET_PATH, on_bad_lines='skip', engine='python')
    print(f"‚úÖ '{FACT_FAKES_TEST_DATASET_PATH}' ({len(df_test)} “õ–∞—Ç–∞—Ä) –∂“Ø–∫—Ç–µ–ª–¥—ñ.")

    # NaN –º”ô–Ω–¥–µ—Ä—ñ–Ω –∂–æ—é
    df_test.dropna(subset=['text', 'label'], inplace=True)

    # –¢–µ—Å—Ç –¥–µ—Ä–µ–∫—Ç–µ—Ä—ñ–Ω ”©“£–¥–µ—É (real/fake –∫“Ø—Ç–µ–º—ñ–∑)
    df_test['label'] = df_test['label'].astype(str).str.strip().str.lower() # –ö—ñ—à—ñ ”ô—Ä—ñ–ø–∫–µ –∫–µ–ª—Ç—ñ—Ä–µ–º—ñ–∑
    df_test['label_numeric'] = df_test['label'].map(label_map_test)

    # –ú–∞“£—ã–∑–¥—ã: –¢“Ø—Ä–ª–µ–Ω–¥—ñ—Ä—É –º“Ø–º–∫—ñ–Ω –±–æ–ª–º–∞“ì–∞–Ω (NaN) –∂–æ–ª–¥–∞—Ä–¥—ã –∂–æ—è–º—ã–∑
    original_test_count = len(df_test)
    df_test.dropna(subset=['label_numeric'], inplace=True)
    dropped_test = original_test_count - len(df_test)
    if dropped_test > 0:
         print(f"‚ö†Ô∏è –ï–°–ö–ï–†–¢–£: –¢–µ—Å—Ç —Ñ–∞–π–ª—ã–Ω–∞–Ω {dropped_test} –∂–æ–ª –∂–∞—Ä–∞–º—Å—ã–∑ –±–µ–ª–≥—ñ ('real'/'fake'-—Ç–µ–Ω –±–∞—Å“õ–∞) –±–æ–ª“ì–∞–Ω–¥—ã“õ—Ç–∞–Ω –∂–æ–π—ã–ª–¥—ã.")

    X_test = df_test['text'].tolist()
    y_test = df_test['label_numeric'].astype(int).tolist()

    if not X_test:
        print(f"üõë –ö–†–ò–¢–ò–ö–ê–õ–´“ö “ö–ê–¢–ï: –¢–µ—Å—Ç –º”ô—Ç—ñ–Ω–¥–µ—Ä—ñ–Ω—ñ“£ —Ç—ñ–∑—ñ–º—ñ (X_test) –±–æ—Å! '{FACT_FAKES_TEST_DATASET_PATH}' —Ñ–∞–π–ª—ã–Ω —Ç–µ–∫—Å–µ—Ä—ñ“£—ñ–∑.")
        exit()

    print(f"–¢–µ—Å—Ç –∂–∏—ã–Ω—Ç—ã“ì—ã –¥–∞–π—ã–Ω: {len(X_test)} –º”ô—Ç—ñ–Ω")
    print(f"–¢–µ—Å—Ç—Ç–µ–≥—ñ –±–µ–ª–≥—ñ–ª–µ—Ä (0/1): {pd.Series(y_test).value_counts().to_dict()}")


except FileNotFoundError as e:
    print(f"üõë “ö–ê–¢–ï: –¢–µ—Å—Ç —Ñ–∞–π–ª—ã '{FACT_FAKES_TEST_DATASET_PATH}' —Ç–∞–±—ã–ª–º–∞–¥—ã. –ê–ª–¥—ã–º–µ–Ω –æ–Ω—ã –∂–∞—Å–∞“£—ã–∑!")
    print("–ù“±—Å“õ–∞—É–ª—ã“õ: –®—ã–Ω–∞–π—ã –∂–∞“£–∞–ª—ã“õ—Ç–∞—Ä–¥–∞–Ω —Ñ–∞–∫—Ç—ñ–ª–µ—Ä–¥—ñ ”©–∑–≥–µ—Ä—Ç—ñ–ø, —Ñ–µ–π–∫ –¥–µ–ø –±–µ–ª–≥—ñ–ª–µ“£—ñ–∑.")
    exit()
except KeyError as e:
    print(f"üõë “ö–ê–¢–ï: '{FACT_FAKES_TEST_DATASET_PATH}' —Ñ–∞–π–ª—ã–Ω–¥–∞ '{e}' –±–∞“ì–∞–Ω—ã –∂–æ“õ. CSV —Ñ–∞–π–ª—ã–Ω —Ç–µ–∫—Å–µ—Ä—ñ“£—ñ–∑ ('text', 'label'). –°–∫—Ä–∏–ø—Ç —Ç–æ“õ—Ç–∞—Ç—ã–ª–¥—ã.")
    exit()
except Exception as e:
     print(f"üõë –î–µ—Ä–µ–∫—Ç–µ—Ä–¥—ñ –∂“Ø–∫—Ç–µ—É –∫–µ–∑—ñ–Ω–¥–µ “õ–∞—Ç–µ: {e}")
     exit()

# ===============================================================
# DL –ú–û–î–ï–õ–¨–î–Ü (Transformers Pipeline) –§–ê–ö–¢-–§–ï–ô–ö–¢–ï–†–ú–ï–ù –¢–ï–°–¢–Ü–õ–ï–£
# ===============================================================
print("\n--- DL –ú–û–î–ï–õ–¨–î–Ü –¢–ï–°–¢–Ü–õ–ï–£ ---")
print(f"DL –ú–æ–¥–µ–ª—ñ–Ω –∂“Ø–∫—Ç–µ—É: {MODEL_PATH}")
print(f"“ö“±—Ä—ã–ª“ì—ã: {'GPU' if DEVICE == 0 else 'CPU'}")
dl_preds = [] # –ë–æ–ª–∂–∞–º–¥–∞—Ä —Ç—ñ–∑—ñ–º—ñ
dl_pipeline = None # –ê–ª–¥—ã–Ω –∞–ª–∞ –∞–Ω—ã“õ—Ç–∞–π–º—ã–∑

try:
    dl_pipeline = pipeline(
        "text-classification",
        model=MODEL_PATH,
        tokenizer=MODEL_PATH,
        device=DEVICE
    )
    print("‚úÖ DL –º–æ–¥–µ–ª—å –∂“Ø–∫—Ç–µ–ª–¥—ñ.")

    print(f"DL –º–æ–¥–µ–ª—å {len(X_test)} —Ñ–∞–∫—Ç-—Ñ–µ–π–∫ —Ç–µ—Å—Ç –º”ô—Ç—ñ–Ω—ñ–Ω ”©“£–¥–µ—É–¥–µ...")
    dl_results = []
    batch_size = 16 if DEVICE == 0 else 1
    for out in tqdm(dl_pipeline(X_test, batch_size=batch_size, truncation=True, max_length=512), total=len(X_test)):
        dl_results.append(out)

    dl_preds = [parse_dl_label(res) for res in dl_results]

    print(f"\n--- DL –ú–û–î–ï–õ–¨ –ù”ò–¢–ò–ñ–ï–õ–ï–†–Ü ({FACT_FAKES_TEST_DATASET_PATH}) ---")
    print(classification_report(y_test, dl_preds, target_names=['FAKE (0)', 'REAL (1)']))

except Exception as e:
    print(f"üõë DL –º–æ–¥–µ–ª—å–¥—ñ –∂“Ø–∫—Ç–µ—É –Ω–µ–º–µ—Å–µ —Ç–µ—Å—Ç—ñ–ª–µ—É –∫–µ–∑—ñ–Ω–¥–µ “õ–∞—Ç–µ: {e}")

print("\n--- –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –ê–Ø“ö–¢–ê–õ–î–´ ---")
