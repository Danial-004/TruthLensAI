# test_deep_analysis.py (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)
import os
import sys
from pprint import pprint
from dotenv import load_dotenv

# --- –ì–õ–ê–í–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ó–ê–ì–†–£–ñ–ê–ï–ú .env –§–ê–ô–õ ---
print("–ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞...")
load_dotenv()

# –î–æ–±–∞–≤–ª—è–µ–º –ø–∞–ø–∫—É backend –≤ –ø—É—Ç—å, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥—É–ª–∏
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

# –¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—à–∏ –º–æ–¥—É–ª–∏
from model import FakeNewsDetector
from search_api import WebSearcher
from utils import preprocess_text, get_final_verdict, generate_explanation

def test_analysis(text_to_analyze):
    print("="*50)
    print("üöÄ –ù–ê–ß–ò–ù–ê–ï–ú –î–ò–ê–ì–ù–û–°–¢–ò–ö–£ DEEP ANALYSIS")
    print("="*50)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞ –º–µ—Å—Ç–µ –ª–∏ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    model_path = "backend/models/trained_model"
    if not os.path.exists(model_path):
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ü–∞–ø–∫–∞ —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {model_path}")
        print("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã —Å–∫–∞—á–∞–ª–∏, —Ä–∞—Å–ø–∞–∫–æ–≤–∞–ª–∏, –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–ª–∏ –∏ –ø–æ–ª–æ–∂–∏–ª–∏ –º–æ–¥–µ–ª—å –≤ –Ω—É–∂–Ω–æ–µ –º–µ—Å—Ç–æ.")
        return
        
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API-–∫–ª—é—á–∞
    if not os.getenv("SERP_API_KEY"):
        print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: SERP_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ. –í–µ–±-–ø–æ–∏—Å–∫ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥.")


    try:
        print("\n[–®–∞–≥ 1/4] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
        detector = FakeNewsDetector()
        searcher = WebSearcher()
        print("‚úÖ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã.")
    except Exception as e:
        print(f"‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        return

    print(f"\n[–®–∞–≥ 2/4] –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞...")
    clean_text = preprocess_text(text_to_analyze)
    initial_prediction = detector.predict(clean_text)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢ –ü–ï–†–í–ò–ß–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê (–û–ë–£–ß–ï–ù–ù–ê–Ø –ú–û–î–ï–õ–¨):")
    pprint(initial_prediction)

    print("\n[–®–∞–≥ 3/4] –ü–æ–∏—Å–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ...")
    search_results = searcher.search(clean_text, language='en', max_results=5)
    if not search_results:
        print("‚ö†Ô∏è –ü–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")
    else:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(search_results)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.")

    print("\n[–®–∞–≥ 4/4] –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é NLI-–º–æ–¥–µ–ª–∏...")
    ranked_sources = detector.rank_sources_nli(clean_text, search_results)
    
    if not ranked_sources:
        print("‚ö†Ô∏è –ü–æ—Å–ª–µ NLI-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.")
    else:
        print("–†–ï–ó–£–õ–¨–¢–ê–¢ NLI-–†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–Ø (–æ—Ç –ª—É—á—à–µ–≥–æ –∫ —Ö—É–¥—à–µ–º—É):")
        for source in ranked_sources:
            print(f"  - –ò—Å—Ç–æ—á–Ω–∏–∫: {source.get('title')}, –ë–ê–õ–õ NLI: {source.get('relevance', 'N/A'):.2f}")
    
    # --- –§–ò–ù–ê–õ–¨–ù–´–ô –í–ï–†–î–ò–ö–¢ ---
    final_verdict = get_final_verdict(initial_prediction, ranked_sources, original_text=text_to_analyze)
    final_explanation = generate_explanation(final_verdict, 'en') # –Ø–∑—ã–∫ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
    
    print("\n" + "="*50)
    print("üèÅ –§–ò–ù–ê–õ–¨–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:")
    print("="*50)
    print(f"–í–µ—Ä–¥–∏–∫—Ç: {final_verdict.get('classification')}")
    print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {final_verdict.get('confidence'):.2%}")
    print(f"–û–±—ä—è—Å–Ω–µ–Ω–∏–µ: {final_explanation}")


if __name__ == "__main__":
    example_text = "New Study Claims Smartphones May Affect Long-Term Memory"
    test_analysis(example_text)