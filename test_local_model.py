import pandas as pd
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from transformers import pipeline # DL –º–æ–¥–µ–ª—å–¥—ñ –∂“Ø–∫—Ç–µ—É “Ø—à—ñ–Ω
import torch
import warnings
from tqdm import tqdm

# --- –ü–∞—Ä–∞–º–µ—Ç—Ä–ª–µ—Ä ---
# –§–∞–π–ª –∂–æ–ª–¥–∞—Ä—ã
TRAINING_DATASET_PATH = "final_training_dataset_kz.csv" # ML “Ø–π—Ä–µ—Ç—É “Ø—à—ñ–Ω
TEST_DATASET_PATH = "test.csv" # ML –∂”ô–Ω–µ DL —Ç–µ—Å—Ç “Ø—à—ñ–Ω
relative_model_path = os.path.join("backend", "models", "truthlens_kk_model") # DL –º–æ–¥–µ–ª—å
MODEL_PATH = os.path.abspath(relative_model_path)
DEVICE = 0 if torch.cuda.is_available() else -1 # DL “Ø—à—ñ–Ω

# ML –º–æ–¥–µ–ª—å –ø–∞—Ä–∞–º–µ—Ç—Ä–ª–µ—Ä—ñ
MAX_FEATURES_TFIDF = 10000 # TF-IDF “Ø—à—ñ–Ω –µ“£ –∂–∏—ñ –∫–µ–∑–¥–µ—Å–µ—Ç—ñ–Ω —Å”©–∑–¥–µ—Ä —Å–∞–Ω—ã

# –ï—Å–∫–µ—Ä—Ç—É–ª–µ—Ä–¥—ñ ”©—à—ñ—Ä—É
warnings.filterwarnings("ignore")
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ‚úÖ‚úÖ‚úÖ ”®–ó–ì–ï–†–Ü–°: –¢–µ—Å—Ç —Ñ–∞–π–ª—ã “Ø—à—ñ–Ω label_map “ö–ê–ô–¢–ê–†–´–õ–î–´ ‚úÖ‚úÖ‚úÖ
label_map_test = {'real': 1, 'fake': 0}

# DL –º–æ–¥–µ–ª—å –±–æ–ª–∂–∞–º–¥–∞—Ä—ã–Ω —Å–∞–Ω–¥–∞—Ä“ì–∞ –∞—É—ã—Å—Ç—ã—Ä—É —Ñ—É–Ω–∫—Ü–∏—è—Å—ã (”©–∑–≥–µ—Ä—ñ—Å—Å—ñ–∑)
def parse_dl_label(d):
    return 1 if d['label'] == 'LABEL_1' else 0

print("--- –î–ï–†–ï–ö–¢–ï–†–î–Ü –ñ“Æ–ö–¢–ï–£ ---")
try:
    df_train = pd.read_csv(TRAINING_DATASET_PATH, on_bad_lines='skip', engine='python')
    df_test = pd.read_csv(TEST_DATASET_PATH, on_bad_lines='skip', engine='python')
    print(f"‚úÖ '{TRAINING_DATASET_PATH}' ({len(df_train)} “õ–∞—Ç–∞—Ä) –∂“Ø–∫—Ç–µ–ª–¥—ñ.")
    print(f"‚úÖ '{TEST_DATASET_PATH}' ({len(df_test)} “õ–∞—Ç–∞—Ä) –∂“Ø–∫—Ç–µ–ª–¥—ñ.")

    # --- –¢—Ä–µ–Ω–∏–Ω–≥ –¥–µ—Ä–µ–∫—Ç–µ—Ä—ñ–Ω ”©“£–¥–µ—É (1/0 –∫“Ø—Ç–µ–º—ñ–∑) ---
    df_train.dropna(subset=['text', 'label'], inplace=True)
    df_train['label_numeric'] = pd.to_numeric(df_train['label'], errors='coerce')
    original_train_count = len(df_train)
    df_train.dropna(subset=['label_numeric'], inplace=True)
    dropped_train = original_train_count - len(df_train)
    if dropped_train > 0:
        print(f"‚ö†Ô∏è –ï–°–ö–ï–†–¢–£: –¢—Ä–µ–Ω–∏–Ω–≥ —Ñ–∞–π–ª—ã–Ω–∞–Ω {dropped_train} –∂–æ–ª –∂–∞—Ä–∞–º—Å—ã–∑ –±–µ–ª–≥—ñ ('1'/'0'-–¥–µ–Ω –±–∞—Å“õ–∞) –±–æ–ª“ì–∞–Ω–¥—ã“õ—Ç–∞–Ω –∂–æ–π—ã–ª–¥—ã.")

    # --- –¢–µ—Å—Ç –¥–µ—Ä–µ–∫—Ç–µ—Ä—ñ–Ω ”©“£–¥–µ—É (real/fake –∫“Ø—Ç–µ–º—ñ–∑) ---
    df_test.dropna(subset=['text', 'label'], inplace=True)
    # ‚úÖ‚úÖ‚úÖ ”®–ó–ì–ï–†–Ü–°: –¢–µ—Å—Ç “Ø—à—ñ–Ω .map(label_map_test) “õ–æ–ª–¥–∞–Ω–∞–º—ã–∑ ‚úÖ‚úÖ‚úÖ
    df_test['label'] = df_test['label'].astype(str).str.strip().str.lower() # –ö—ñ—à—ñ ”ô—Ä—ñ–ø–∫–µ –∫–µ–ª—Ç—ñ—Ä–µ–º—ñ–∑
    df_test['label_numeric'] = df_test['label'].map(label_map_test)
    original_test_count = len(df_test)
    df_test.dropna(subset=['label_numeric'], inplace=True)
    dropped_test = original_test_count - len(df_test)
    if dropped_test > 0:
         print(f"‚ö†Ô∏è –ï–°–ö–ï–†–¢–£: –¢–µ—Å—Ç —Ñ–∞–π–ª—ã–Ω–∞–Ω {dropped_test} –∂–æ–ª –∂–∞—Ä–∞–º—Å—ã–∑ –±–µ–ª–≥—ñ ('real'/'fake'-—Ç–µ–Ω –±–∞—Å“õ–∞) –±–æ–ª“ì–∞–Ω–¥—ã“õ—Ç–∞–Ω –∂–æ–π—ã–ª–¥—ã.")
    # ‚úÖ‚úÖ‚úÖ ”®–ó–ì–ï–†–Ü–° –ê–Ø“ö–¢–ê–õ–î–´ ‚úÖ‚úÖ‚úÖ

    X_train = df_train['text'].tolist()
    y_train = df_train['label_numeric'].astype(int).tolist()
    X_test = df_test['text'].tolist()
    y_test = df_test['label_numeric'].astype(int).tolist()

    # –¢–µ–∫—Å–µ—Ä—É–ª–µ—Ä
    if not X_train:
         print(f"üõë –ö–†–ò–¢–ò–ö–ê–õ–´“ö “ö–ê–¢–ï: –¢—Ä–µ–Ω–∏–Ω–≥ –º”ô—Ç—ñ–Ω–¥–µ—Ä—ñ–Ω—ñ“£ —Ç—ñ–∑—ñ–º—ñ (X_train) –±–æ—Å! '{TRAINING_DATASET_PATH}' —Ñ–∞–π–ª—ã–Ω —Ç–µ–∫—Å–µ—Ä—ñ“£—ñ–∑.")
         exit()
    # ‚úÖ‚úÖ‚úÖ ”®–ó–ì–ï–†–Ü–°: –¢–µ—Å—Ç –¥–µ—Ä–µ–∫—Ç–µ—Ä—ñ –±–æ—Å –±–æ–ª—Å–∞, “õ–∞—Ç–µ —à—ã“ì–∞—Ä—ã–ø —Ç–æ“õ—Ç–∞—Ç–∞–º—ã–∑ ‚úÖ‚úÖ‚úÖ
    if not X_test:
        print(f"üõë –ö–†–ò–¢–ò–ö–ê–õ–´“ö “ö–ê–¢–ï: –¢–µ—Å—Ç –º”ô—Ç—ñ–Ω–¥–µ—Ä—ñ–Ω—ñ“£ —Ç—ñ–∑—ñ–º—ñ (X_test) –±–æ—Å! '{TEST_DATASET_PATH}' —Ñ–∞–π–ª—ã–Ω–¥–∞“ì—ã 'label' –±–∞“ì–∞–Ω—ã–Ω–¥–∞ 'real' –Ω–µ–º–µ—Å–µ 'fake' –º”ô–Ω–¥–µ—Ä—ñ –∂–æ“õ –Ω–µ–º–µ—Å–µ –¥“±—Ä—ã—Å –æ“õ—ã–ª–º–∞–¥—ã.")
        exit()

    print(f"–¢—Ä–µ–Ω–∏–Ω–≥ –∂–∏—ã–Ω—Ç—ã“ì—ã –¥–∞–π—ã–Ω: {len(X_train)} –º”ô—Ç—ñ–Ω")
    print(f"–¢–µ—Å—Ç –∂–∏—ã–Ω—Ç—ã“ì—ã –¥–∞–π—ã–Ω: {len(X_test)} –º”ô—Ç—ñ–Ω")

    print(f"–¢—Ä–µ–Ω–∏–Ω–≥—Ç–µ–≥—ñ –±–µ–ª–≥—ñ–ª–µ—Ä (0/1): {pd.Series(y_train).value_counts().to_dict()}")
    print(f"–¢–µ—Å—Ç—Ç–µ–≥—ñ –±–µ–ª–≥—ñ–ª–µ—Ä (0/1): {pd.Series(y_test).value_counts().to_dict()}") # –ï–Ω–¥—ñ –±–æ—Å –±–æ–ª–º–∞—É—ã –∫–µ—Ä–µ–∫


except FileNotFoundError as e:
    print(f"üõë “ö–ê–¢–ï: –§–∞–π–ª —Ç–∞–±—ã–ª–º–∞–¥—ã: {e}. –°–∫—Ä–∏–ø—Ç —Ç–æ“õ—Ç–∞—Ç—ã–ª–¥—ã.")
    exit()
except KeyError as e:
    print(f"üõë “ö–ê–¢–ï: –§–∞–π–ª–¥–∞ '{e}' –±–∞“ì–∞–Ω—ã –∂–æ“õ. CSV —Ñ–∞–π–ª—ã–Ω —Ç–µ–∫—Å–µ—Ä—ñ“£—ñ–∑ ('text', 'label'). –°–∫—Ä–∏–ø—Ç —Ç–æ“õ—Ç–∞—Ç—ã–ª–¥—ã.")
    exit()
except Exception as e:
     print(f"üõë –î–µ—Ä–µ–∫—Ç–µ—Ä–¥—ñ –∂“Ø–∫—Ç–µ—É –∫–µ–∑—ñ–Ω–¥–µ “õ–∞—Ç–µ: {e}")
     exit()

# ===============================================================
# ML –ú–û–î–ï–õ–¨ (TF-IDF + Logistic Regression)
# ===============================================================
print("\n--- ML –ú–û–î–ï–õ–¨–î–Ü “Æ–ô–†–ï–¢–£ –ñ”ò–ù–ï –¢–ï–°–¢–Ü–õ–ï–£ ---")
ml_preds = [] # –ë–æ–ª–∂–∞–º–¥–∞—Ä —Ç—ñ–∑—ñ–º—ñ
# ML –±”©–ª—ñ–º—ñ –µ–Ω–¥—ñ –æ—Ä—ã–Ω–¥–∞–ª—É—ã –∫–µ—Ä–µ–∫, —Å–µ–±–µ–±—ñ X_train –∂”ô–Ω–µ X_test –±–æ—Å –µ–º–µ—Å
print("TF-IDF –≤–µ–∫—Ç–æ—Ä–ª–∞—É—ã—à—ã–Ω “õ“±—Ä—É...")
vectorizer = TfidfVectorizer(max_features=MAX_FEATURES_TFIDF, ngram_range=(1, 2))
print("TF-IDF: –¢—Ä–µ–Ω–∏–Ω–≥ –¥–µ—Ä–µ–∫—Ç–µ—Ä—ñ–Ω —Ç“Ø—Ä–ª–µ–Ω–¥—ñ—Ä—É...")
X_train_tfidf = vectorizer.fit_transform(X_train)
print(f"TF-IDF: –¢—Ä–µ–Ω–∏–Ω–≥ –≤–µ–∫—Ç–æ—Ä—ã–Ω—ã“£ ”©–ª—à–µ–º—ñ: {X_train_tfidf.shape}")

print("Logistic Regression –º–æ–¥–µ–ª—ñ–Ω “Ø–π—Ä–µ—Ç—É...")
ml_model = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced')
ml_model.fit(X_train_tfidf, y_train)
print("‚úÖ ML –º–æ–¥–µ–ª—å “Ø–π—Ä–µ—Ç—ñ–ª–¥—ñ.")

print("TF-IDF: –¢–µ—Å—Ç –¥–µ—Ä–µ–∫—Ç–µ—Ä—ñ–Ω —Ç“Ø—Ä–ª–µ–Ω–¥—ñ—Ä—É...")
X_test_tfidf = vectorizer.transform(X_test)

print("ML –º–æ–¥–µ–ª—å–º–µ–Ω —Ç–µ—Å—Ç –¥–µ—Ä–µ–∫—Ç–µ—Ä—ñ–Ω–¥–µ –±–æ–ª–∂–∞–º –∂–∞—Å–∞—É...")
ml_preds = ml_model.predict(X_test_tfidf)

print("\n--- ML –ú–û–î–ï–õ–¨ –ù”ò–¢–ò–ñ–ï–õ–ï–†–Ü (test.csv) ---")
print(classification_report(y_test, ml_preds, target_names=['FAKE (0)', 'REAL (1)']))

# ===============================================================
# DL –ú–û–î–ï–õ–¨ (Transformers Pipeline)
# ===============================================================
print("\n--- DL –ú–û–î–ï–õ–¨–î–Ü –¢–ï–°–¢–Ü–õ–ï–£ (test.csv) ---")
print(f"DL –ú–æ–¥–µ–ª—ñ–Ω –∂“Ø–∫—Ç–µ—É: {MODEL_PATH}")
print(f"“ö“±—Ä—ã–ª“ì—ã: {'GPU' if DEVICE == 0 else 'CPU'}")
dl_preds = [] # –ë–æ–ª–∂–∞–º–¥–∞—Ä —Ç—ñ–∑—ñ–º—ñ
dl_pipeline = None # –ê–ª–¥—ã–Ω –∞–ª–∞ –∞–Ω—ã“õ—Ç–∞–π–º—ã–∑

try:
    dl_pipeline = pipeline(
        "text-classification",
        model=MODEL_PATH,
        tokenizer=MODEL_PATH,
        device=DEVICE
    )
    print("‚úÖ DL –º–æ–¥–µ–ª—å –∂“Ø–∫—Ç–µ–ª–¥—ñ.")

    # DL —Ç–µ—Å—Ç—ñ–ª–µ—É –±”©–ª—ñ–º—ñ –µ–Ω–¥—ñ –æ—Ä—ã–Ω–¥–∞–ª—É—ã –∫–µ—Ä–µ–∫
    print(f"DL –º–æ–¥–µ–ª—å {len(X_test)} —Ç–µ—Å—Ç –º”ô—Ç—ñ–Ω—ñ–Ω ”©“£–¥–µ—É–¥–µ...")
    dl_results = []
    batch_size = 16 if DEVICE == 0 else 1
    for out in tqdm(dl_pipeline(X_test, batch_size=batch_size, truncation=True, max_length=512), total=len(X_test)):
        dl_results.append(out)

    dl_preds = [parse_dl_label(res) for res in dl_results]

    print("\n--- DL –ú–û–î–ï–õ–¨ –ù”ò–¢–ò–ñ–ï–õ–ï–†–Ü (test.csv) ---")
    print(classification_report(y_test, dl_preds, target_names=['FAKE (0)', 'REAL (1)']))


except Exception as e:
    print(f"üõë DL –º–æ–¥–µ–ª—å–¥—ñ –∂“Ø–∫—Ç–µ—É –Ω–µ–º–µ—Å–µ —Ç–µ—Å—Ç—ñ–ª–µ—É –∫–µ–∑—ñ–Ω–¥–µ “õ–∞—Ç–µ: {e}")

print("\n--- –°–ê–õ–´–°–¢–´–†–£ –ê–Ø“ö–¢–ê–õ–î–´ ---")


# ===============================================================
# –ß–ê–°–¢–¨ 2: –†–£–ß–ù–û–ô "–°–õ–û–ñ–ù–´–ô" –¢–ï–°–¢
# ===============================================================
print("\n" + "="*50)
print("--- –ß–ê–°–¢–¨ 2: –†–£–ß–ù–û–ô –°–¢–†–ï–°–°-–¢–ï–°–¢ ---")
print("–í–≤–µ–¥–∏ –ª—é–±–æ–π —Ç–µ–∫—Å—Ç –Ω–∞ –∫–∞–∑–∞—Ö—Å–∫–æ–º. –î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏ 'exit' –∏–ª–∏ '—à—ã“ì—É'.")
print("="*50)

while True:
    try:
        text = input("\n[–¢–í–û–ô –¢–ï–ö–°–¢]: ")
        if text.lower() in ['exit', 'quit', '—à—ã“ì—É', '—Å—Ç–æ–ø']:
            break

        if len(text) < 10:
            print("–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç, –≤–≤–µ–¥–∏ —á—Ç–æ-—Ç–æ –ø–æ—Å–µ—Ä—å–µ–∑–Ω–µ–µ.")
            continue

        # --- DL –ú–æ–¥–µ–ª—å –ë–æ–ª–∂–∞–º—ã ---
        if dl_pipeline: # –ï–≥–µ—Ä DL –º–æ–¥–µ–ª—å –∂“Ø–∫—Ç–µ–ª—Å–µ
            result_dl = dl_pipeline(text)[0]
            label_dl = result_dl['label']
            score_dl = result_dl['score'] * 100
            print("\n--- –í–ï–†–î–ò–ö–¢ DL –ú–û–î–ï–õ–ò ---")
            if label_dl == 'LABEL_1':
                print(f"‚úÖ –†–ï–ê–õ–¨–ù–ê–Ø –ù–û–í–û–°–¢–¨ (–° –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é {score_dl:.2f}%)")
            else:
                print(f"‚ùå –§–ï–ô–ö (–° –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é {score_dl:.2f}%)")
        else:
             print("\n--- –í–ï–†–î–ò–ö–¢ DL –ú–û–î–ï–õ–ò ---")
             print("‚ö†Ô∏è DL –º–æ–¥–µ–ª—å –∂“Ø–∫—Ç–µ–ª–º–µ–≥–µ–Ω.")

        # --- ML –ú–æ–¥–µ–ª—å –ë–æ–ª–∂–∞–º—ã ---
        if 'ml_model' in locals() and 'vectorizer' in locals(): # –ï–≥–µ—Ä ML –º–æ–¥–µ–ª—å “Ø–π—Ä–µ—Ç—ñ–ª—Å–µ
            text_tfidf = vectorizer.transform([text])
            ml_pred_manual = ml_model.predict(text_tfidf)[0]
            ml_proba_manual = ml_model.predict_proba(text_tfidf)[0]
            print("\n--- –í–ï–†–î–ò–ö–¢ ML –ú–û–î–ï–õ–ò ---")
            if ml_pred_manual == 1:
                print(f"‚úÖ –†–ï–ê–õ–¨–ù–ê–Ø –ù–û–í–û–°–¢–¨ (–° –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é {ml_proba_manual[1]*100:.2f}%)")
            else:
                print(f"‚ùå –§–ï–ô–ö (–° –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é {ml_proba_manual[0]*100:.2f}%)")
        else:
            print("\n--- –í–ï–†–î–ò–ö–¢ ML –ú–û–î–ï–õ–ò ---")
            print("‚ö†Ô∏è ML –º–æ–¥–µ–ª—å “Ø–π—Ä–µ—Ç—ñ–ª–º–µ–≥–µ–Ω.")

    except NameError as e:
         print(f"üõë “ö–æ–ª–º–µ–Ω —Ç–µ—Å—Ç—ñ–ª–µ—É –∫–µ–∑—ñ–Ω–¥–µ “õ–∞—Ç–µ: {e}. –ú–æ–¥–µ–ª—å–¥–µ—Ä–¥—ñ“£ –±—ñ—Ä—ñ –¥“±—Ä—ã—Å –∂“Ø–∫—Ç–µ–ª–º–µ–≥–µ–Ω/“Ø–π—Ä–µ—Ç—ñ–ª–º–µ–≥–µ–Ω –±–æ–ª—É—ã –º“Ø–º–∫—ñ–Ω.")
         # break # –ú“Ø–º–∫—ñ–Ω –∂–∞–ª“ì–∞—Å—Ç—ã—Ä–∞ –±–µ—Ä—É –∫–µ—Ä–µ–∫ —à—ã“ì–∞—Ä?
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
    except KeyboardInterrupt:
        break

print("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")